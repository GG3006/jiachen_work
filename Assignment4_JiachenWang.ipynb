{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4 - Using NLP to play the stock market\n",
    "\n",
    "In this assignment, we'll use everything we've learned to analyze corporate news and pick stocks. Be aware that in this assignment, we're trying to beat the benchmark of random chance (aka better than 50%).\n",
    "\n",
    "This assignment will involve building three models:\n",
    "\n",
    "**1. An RNN based on word inputs**\n",
    "\n",
    "**2. A CNN based on character inputs**\n",
    "\n",
    "**3. A neural net architecture that merges the previous two models**\n",
    "\n",
    "You will apply these models to predicting whether a stock return will be positive or negative in the same day of a news publication.\n",
    "\n",
    "## Your X - Reuters news data\n",
    "\n",
    "Reuters is a news outlet that reports on corporations, among many other things. Stored in the `news_reuters.csv` file is news data listed in columns. The corresponding columns are the `ticker`, `name of company`, `date of publication`, `headline`, `first sentence`, and `news category`.\n",
    "\n",
    "In this assignment it is up to you to decide how to clean this dataset. For instance, many of the first sentences contain a location name showing where the reporting is done. This is largely irrevant information and will probably just make your data noisier. You can also choose to subset on a certain news category, which might enhance your model performance and also limit the size of your data.\n",
    "\n",
    "## Your Y - Stock information from Yahoo! Finance\n",
    "\n",
    "Trading data from Yahoo! Finance was collected and then normalized using the [S&P 500](https://en.wikipedia.org/wiki/S%26P_500_Index). This is stored in the `stockReturns.json` file. \n",
    "\n",
    "In our dataset, the ticker for the S&P is `^GSPC`. Each ticker is compared the S&P and then judged on whether it is outperforming (positive value) or under-performing (negative value) the S&P. Each value is reported on a daily interval from 2004 to now.\n",
    "\n",
    "Below is a diagram of the data in the json file. Note there are three types of data: short: 1 day return, mid: 7 day return, long 28 day return.\n",
    "\n",
    "```\n",
    "          term (short/mid/long)\n",
    "         /         |         \\\n",
    "   ticker A   ticker B   ticker C\n",
    "      /   \\      /   \\      /   \\\n",
    "  date1 date2 date1 date2 date1 date2\n",
    "```\n",
    "\n",
    "You will need to pick a length of time to focus on (day, week, month). You are welcome to train models on each dataset as well.  \n",
    "\n",
    "Transform the return data such that the outcome will be binary:\n",
    "\n",
    "```\n",
    "label[y < 0] = 0\n",
    "label[y >= 0] = 1\n",
    "```\n",
    "\n",
    "Finally, this data needs needs to be joined on the date and ticker - For each date of news publication, we want to join the corresponding corporation's news on its return information. We make the assumption that the day's return will reflect the sentiment of the news, regardless of timing.\n",
    "\n",
    "\n",
    "# Your models - RNN, CNN, and RNN+CNN\n",
    "\n",
    "For your RNN model, it needs to be based on word inputs, embedding the word inputs, encoding them with an RNN layer, and finally a decoding step (such as softmax or some other choice).\n",
    "\n",
    "Your CNN model will be based on characters. For reference on how to do this, look at the CNN class demonstration in the course repository.\n",
    "\n",
    "Finally you will combine the architecture for both of these models, either [merging](https://github.com/ShadyF/cnn-rnn-classifier) using the [Functional API](https://keras.io/getting-started/functional-api-guide/) or [stacking](http://www.aclweb.org/anthology/S17-2134). See the links for reference.\n",
    "\n",
    "For each of these models, you will need to:\n",
    "1. Create a train and test set, retaining the same test set for every model\n",
    "2. Show the architecture for each model, printing it in your python notebook\n",
    "2. Report the peformance according to some metric\n",
    "3. Compare the performance of all of these models in a table (precision and recall)\n",
    "4. Look at your labeling and print out the underlying data compared to the labels - for each model print out 2-3 examples of a good classification and a bad classification. Make an assertion why your model does well or poorly on those outputs.\n",
    "5. For each model, calculate the return from the three most probable positive stock returns. Compare it to the actual return. Print this information in a table.\n",
    "\n",
    "### Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "news_reuters=pd.read_csv(\"news_reuters.csv\",header=None)\n",
    "stockReturns=pd.read_json(\"stockReturns.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ticker</th>\n",
       "      <th>Date</th>\n",
       "      <th>Return</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>20040106</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>20040107</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>20040108</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Ticker      Date  Return\n",
       "0   AAPL  20040106       0\n",
       "1   AAPL  20040107       1\n",
       "2   AAPL  20040108       1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stockReturns=stockReturns.iloc[0:len(stockReturns)-1,:] #remove SP500\n",
    "tickers=stockReturns.index.tolist() #collect all tickers\n",
    "\n",
    "#create a data frame that show return relative to SP500 for each ticker at available dates\n",
    "returns=[]\n",
    "for i in range(len(tickers)):\n",
    "    for date, short_return in stockReturns.iloc[i,2].items():\n",
    "        returns.append([tickers[i],date,short_return])\n",
    "\n",
    "for j in range(len(returns)):\n",
    "    if returns[j][2]>=0:\n",
    "        returns[j][2]=1\n",
    "    else:\n",
    "        returns[j][2]=0\n",
    "        \n",
    "returns=pd.DataFrame(returns,columns=[\"Ticker\",\"Date\",\"Return\"])\n",
    "returns.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jiach\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize news data\n",
    "\n",
    "from nltk import word_tokenize\n",
    "\n",
    "news=[]\n",
    "text=[]\n",
    "for i in range(len(news_reuters)):\n",
    "    headline=str(news_reuters.iloc[i,3])\n",
    "    sent=str(news_reuters.iloc[i,4])\n",
    "    total=word_tokenize(headline+sent)\n",
    "    for j in range(len(total)):\n",
    "        text.append(total[j])        \n",
    "    news.append([news_reuters.iloc[i,0],news_reuters.iloc[i,2],total])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ticker</th>\n",
       "      <th>Date</th>\n",
       "      <th>Return</th>\n",
       "      <th>News</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>20110706</td>\n",
       "      <td>1</td>\n",
       "      <td>[Hackers, expose, flaw, in, Apple, iPad, iPhon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>20110706</td>\n",
       "      <td>1</td>\n",
       "      <td>[Hackers, expose, flaw, in, Apple, iPad, iPhon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>20110706</td>\n",
       "      <td>1</td>\n",
       "      <td>[Samsung, estimates, Q2, profit, down, 26, pct...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Ticker      Date  Return                                               News\n",
       "0   AAPL  20110706       1  [Hackers, expose, flaw, in, Apple, iPad, iPhon...\n",
       "1   AAPL  20110706       1  [Hackers, expose, flaw, in, Apple, iPad, iPhon...\n",
       "2   AAPL  20110706       1  [Samsung, estimates, Q2, profit, down, 26, pct..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news=pd.DataFrame(news,columns=[\"Ticker\",\"Date\",\"News\"])\n",
    "news[\"Date\"]=news[\"Date\"].astype(\"str\")\n",
    "data=pd.merge(returns,news,on=[\"Ticker\",\"Date\"])\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 114388 unique words tokens.\n",
      "Using vocabulary size 30000.\n"
     ]
    }
   ],
   "source": [
    "#create a dictionary\n",
    "\n",
    "import nltk\n",
    "\n",
    "vocabulary_size = 30000\n",
    "unknown_token = \"UNKNOWN_TOKEN\"\n",
    "\n",
    "word_freq=nltk.FreqDist(text)\n",
    "print(\"Found %d unique words tokens.\" % len(word_freq.items()))\n",
    "\n",
    "vocab = word_freq.most_common(vocabulary_size-1)\n",
    "index_to_word = [x[0] for x in vocab]\n",
    "index_to_word.append(unknown_token)\n",
    "word_to_index = dict([(w,i) for i,w in enumerate(index_to_word)])\n",
    "print(\"Using vocabulary size %d.\" % vocabulary_size)\n",
    "\n",
    "data=data.values.tolist()\n",
    "\n",
    "length=[]\n",
    "for i in range(len(data)):\n",
    "    length.append(len(data[i][3]))\n",
    "max_length=max(length)\n",
    "    \n",
    "for i in range(len(data)):\n",
    "    for j in range(len(data[i][3])):\n",
    "        if data[i][3][j] in word_to_index:\n",
    "            data[i][3][j]=word_to_index[data[i][3][j]]\n",
    "        else:\n",
    "            data[i][3][j]=word_to_index[unknown_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jiach\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#padding\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "X=[]\n",
    "Y=[]\n",
    "for i in range(len(data)):\n",
    "    X.append(data[i][3])\n",
    "    Y.append(data[i][2])\n",
    "X = pad_sequences(maxlen=max_length, sequences=X, value=0)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, Y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 129, 200)          6000000   \n",
      "_________________________________________________________________\n",
      "simple_rnn_1 (SimpleRNN)     (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 20)                2020      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 6,032,141\n",
      "Trainable params: 6,032,141\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Embedding, Dense, Dropout\n",
    "from keras.layers.recurrent import LSTM, GRU, SimpleRNN\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "\n",
    "model_1=Sequential()\n",
    "model_1.add(Embedding(vocabulary_size, 200, input_length=max_length))\n",
    "model_1.add(SimpleRNN(100))  \n",
    "model_1.add(Dropout(0.2))\n",
    "model_1.add(Dense(20,activation=\"relu\"))\n",
    "model_1.add(Dense(1, activation='sigmoid'))\n",
    "model_1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model_1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19468 samples, validate on 2164 samples\n",
      "Epoch 1/4\n",
      "19468/19468 [==============================] - 32s 2ms/step - loss: 0.6890 - acc: 0.5368 - val_loss: 0.6801 - val_acc: 0.5675\n",
      "Epoch 2/4\n",
      "19468/19468 [==============================] - 31s 2ms/step - loss: 0.5032 - acc: 0.7690 - val_loss: 0.6810 - val_acc: 0.6188\n",
      "Epoch 3/4\n",
      "19468/19468 [==============================] - 30s 2ms/step - loss: 0.2275 - acc: 0.9181 - val_loss: 0.8763 - val_acc: 0.6081\n",
      "Epoch 4/4\n",
      "19468/19468 [==============================] - 31s 2ms/step - loss: 0.1436 - acc: 0.9520 - val_loss: 0.9271 - val_acc: 0.6280\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c05e0698d0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.fit(X_tr, y_tr, batch_size=200, epochs=4, validation_split=0.1,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRECISION: 0.256\n",
      "RECALL: 0.506\n",
      "F1: 0.340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jiach\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\jiach\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "prediction_1=model_1.predict(X_te)\n",
    "prediction_1=np.argmax(prediction_1,axis=-1)\n",
    "\n",
    "precision_1 = precision_score(y_true=y_te, y_pred=prediction_1, average='weighted')\n",
    "recall_1 = recall_score(y_true=y_te, y_pred=prediction_1, average='weighted')\n",
    "f1_1 = f1_score(y_true=y_te, y_pred=prediction_1, average='weighted')\n",
    "\n",
    "print(\"PRECISION: {:.3f}\".format(precision_1))\n",
    "print(\"RECALL: {:.3f}\".format(recall_1))\n",
    "print(\"F1: {:.3f}\".format(f1_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First good example\n",
      "Actual Return: 0\n",
      "Predicted Return: 0\n",
      "Stock news: ['Bristol-Myers', 'pulls', 'U.S.', 'marketing', 'application', 'for', 'hepatitis', 'C', 'treatment', 'Oct', '7', 'Bristol-Myers', 'Squibb', 'said', 'it', 'withdrew', 'its', 'U.S.', 'marketing', 'application', 'for', 'a', 'drug', 'combination', 'treat', 'hepatitis', 'C', '.']\n"
     ]
    }
   ],
   "source": [
    "# show some good and bad example\n",
    "\n",
    "correct=[]\n",
    "incorrect=[]\n",
    "\n",
    "for i in range(len(y_te)):\n",
    "    if prediction_1[i]==y_te[i]:\n",
    "        correct.append(i)\n",
    "    else:\n",
    "        incorrect.append(i)\n",
    "        \n",
    "print(\"First good example\")\n",
    "print(\"Actual Return: %s\" % y_te[correct[0]])\n",
    "print(\"Predicted Return: %s\" % prediction_1[correct[0]])\n",
    "\n",
    "information=[]\n",
    "for i in range(len(X_te[correct[0]])):\n",
    "    if X_te[correct[0]][i]!=0:\n",
    "        information.append(X_te[correct[0]][i])\n",
    "information=[index_to_word[w] for w in information]\n",
    "    \n",
    "print(\"Stock news: %s\" %information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second good example\n",
      "Actual Return: 0\n",
      "Predicted Return: 0\n",
      "Stock news: ['Lehman', 'payout', 'plan', 'OK', \"'d\", 'for', 'creditor', 'vote', 'NEW', 'YORK', 'Creditors', 'of', 'Lehman', 'Brothers', 'Holdings', 'Inc', 'will', 'be', 'allowed', 'vote', 'on', 'the', 'failed', 'bank', \"'s\", '$', '65', 'billion', 'payback', 'plan', 'clearing', 'a', 'major', 'hurdle', 'in', 'the', 'path', 'ending', 'the', 'biggest', 'bankruptcy', 'in', 'U.S.', 'history', '.']\n"
     ]
    }
   ],
   "source": [
    "print(\"Second good example\")\n",
    "print(\"Actual Return: %s\" % y_te[correct[1]])\n",
    "print(\"Predicted Return: %s\" % prediction_1[correct[1]])\n",
    "\n",
    "information=[]\n",
    "for i in range(len(X_te[correct[1]])):\n",
    "    if X_te[correct[1]][i]!=0:\n",
    "        information.append(X_te[correct[1]][i])\n",
    "information=[index_to_word[w] for w in information]\n",
    "    \n",
    "print(\"Stock news: %s\" %information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First bad example\n",
      "Actual Return: 1\n",
      "Predicted Return: 0\n",
      "Stock news: ['Credit', 'Suisse', 'CEO', 'says', 'bank', 'does', 'not', 'need', 'raise', 'UNKNOWN_TOKEN', 'Aug', '9', 'Swiss', 'bank', 'Credit', 'Suisse', 'AG', 'does', 'not', 'need', 'raise', 'capital', '``', 'in', 'most', 'foreseeable', 'scenarios', \"''\", 'Chief', 'Executive', 'Tidjane', 'Thiam', 'said', 'in', 'an', 'interview', 'with', 'Bloomberg', '.']\n"
     ]
    }
   ],
   "source": [
    "print(\"First bad example\")\n",
    "print(\"Actual Return: %s\" % y_te[incorrect[0]])\n",
    "print(\"Predicted Return: %s\" % prediction_1[incorrect[0]])\n",
    "\n",
    "information=[]\n",
    "for i in range(len(X_te[incorrect[0]])):\n",
    "    if X_te[incorrect[0]][i]!=0:\n",
    "        information.append(X_te[incorrect[0]][i])\n",
    "information=[index_to_word[w] for w in information]\n",
    "    \n",
    "print(\"Stock news: %s\" %information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second bad example\n",
      "Actual Return: 1\n",
      "Predicted Return: 0\n",
      "Stock news: ['US', 'STOCKS-Apple', 'lifts', 'Nasdaq', ';', 'Ukraine', 'drags', 'on', 'broader', 'market', '*', 'Apple', 'rallies', 'a', 'day', 'after', 'announcing', '7-for-1', 'stock', 'split']\n"
     ]
    }
   ],
   "source": [
    "print(\"Second bad example\")\n",
    "print(\"Actual Return: %s\" % y_te[incorrect[1]])\n",
    "print(\"Predicted Return: %s\" % prediction_1[incorrect[1]])\n",
    "\n",
    "information=[]\n",
    "for i in range(len(X_te[incorrect[1]])):\n",
    "    if X_te[incorrect[1]][i]!=0:\n",
    "        information.append(X_te[incorrect[1]][i])\n",
    "information=[index_to_word[w] for w in information]\n",
    "    \n",
    "print(\"Stock news: %s\" %information)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\jiach\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:497: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NHWC is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`NHWC` for data_format is deprecated, use `NWC` instead\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 129, 200)          6000000   \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 127, 30)           18030     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 63, 30)            0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1890)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                18910     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 6,036,951\n",
      "Trainable params: 6,036,951\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv1D, MaxPooling1D, Flatten\n",
    "model_2 = Sequential()\n",
    "model_2.add(Embedding(input_dim =vocabulary_size, output_dim = 200, input_length = max_length))\n",
    "model_2.add(Conv1D(filters = 30, kernel_size = 3, strides = 1, padding = 'valid'))\n",
    "model_2.add(MaxPooling1D(2, padding = 'valid'))\n",
    "model_2.add(Flatten())\n",
    "model_2.add(Dense(10,activation=\"relu\"))\n",
    "model_2.add(Dense(1,activation=\"sigmoid\"))\n",
    "model_2.compile(loss='binary_crossentropy', optimizer='adam' , metrics=['accuracy'])\n",
    "print(model_2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19468 samples, validate on 2164 samples\n",
      "Epoch 1/4\n",
      "19468/19468 [==============================] - 19s 984us/step - loss: 0.6913 - acc: 0.5245 - val_loss: 0.6832 - val_acc: 0.5943\n",
      "Epoch 2/4\n",
      "19468/19468 [==============================] - 19s 965us/step - loss: 0.5282 - acc: 0.7827 - val_loss: 0.6421 - val_acc: 0.6382\n",
      "Epoch 3/4\n",
      "19468/19468 [==============================] - 19s 979us/step - loss: 0.1955 - acc: 0.9408 - val_loss: 0.7232 - val_acc: 0.6483\n",
      "Epoch 4/4\n",
      "19468/19468 [==============================] - 20s 1ms/step - loss: 0.1298 - acc: 0.9599 - val_loss: 0.7309 - val_acc: 0.6539\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c063a93dd8>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2.fit(X_tr, y_tr, batch_size=200, epochs=4, validation_split=0.1,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRECISION: 0.256\n",
      "RECALL: 0.506\n",
      "F1: 0.340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jiach\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\jiach\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "prediction_2=model_2.predict(X_te)\n",
    "prediction_2=np.argmax(prediction_2,axis=-1)\n",
    "\n",
    "precision_2 = precision_score(y_true=y_te, y_pred=prediction_2, average='weighted')\n",
    "recall_2 = recall_score(y_true=y_te, y_pred=prediction_2, average='weighted')\n",
    "f1_2 = f1_score(y_true=y_te, y_pred=prediction_2, average='weighted')\n",
    "\n",
    "print(\"PRECISION: {:.3f}\".format(precision_2))\n",
    "print(\"RECALL: {:.3f}\".format(recall_2))\n",
    "print(\"F1: {:.3f}\".format(f1_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First good example\n",
      "Actual Return: 0\n",
      "Predicted Return: 0\n",
      "Stock news: ['AstraZeneca', 'wins', 'U.S.', 'approval', 'for', 'longer', 'use', 'of', 'blood', 'thinner', 'U.S.', 'regulators', 'have', 'approved', 'a', 'new', 'dose', 'of', 'AstraZeneca', \"'s\", 'blood', 'thinner', 'Brilinta', 'for', 'longer-term', 'use', 'in', 'patients', 'with', 'a', 'history', 'of', 'heart', 'attacks', 'boosting', 'prospects', 'for', 'a', 'drug', 'the', 'company', 'thinks', 'will', 'eventually', 'sell', '$', '3.5', 'billion', 'a', 'year', '.']\n"
     ]
    }
   ],
   "source": [
    "correct=[]\n",
    "incorrect=[]\n",
    "\n",
    "for i in range(len(y_te)):\n",
    "    if prediction_2[i]==y_te[i]:\n",
    "        correct.append(i)\n",
    "    else:\n",
    "        incorrect.append(i)\n",
    "        \n",
    "print(\"First good example\")\n",
    "print(\"Actual Return: %s\" % y_te[correct[4]])\n",
    "print(\"Predicted Return: %s\" % prediction_2[correct[4]])\n",
    "\n",
    "information=[]\n",
    "for i in range(len(X_te[correct[4]])):\n",
    "    if X_te[correct[4]][i]!=0:\n",
    "        information.append(X_te[correct[4]][i])\n",
    "information=[index_to_word[w] for w in information]\n",
    "    \n",
    "print(\"Stock news: %s\" %information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second good example\n",
      "Actual Return: 0\n",
      "Predicted Return: 0\n",
      "Stock news: ['BRIEF-Bristol-Myers', 'says', 'EU', 'advanced', 'melanoma', 'drug', '*', 'European', 'Commission', 'UNKNOWN_TOKEN', 'The', 'First', 'And', 'Only', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN', 'Bristol-Myers', 'Squibb', \"'s\", 'UNKNOWN_TOKEN', '(', 'UNKNOWN_TOKEN', ')', '+', 'UNKNOWN_TOKEN', '(', 'UNKNOWN_TOKEN', ')', 'UNKNOWN_TOKEN', 'For', 'Treatment', 'Of', 'Advanced', 'UNKNOWN_TOKEN']\n"
     ]
    }
   ],
   "source": [
    "print(\"Second good example\")\n",
    "print(\"Actual Return: %s\" % y_te[correct[3]])\n",
    "print(\"Predicted Return: %s\" % prediction_2[correct[3]])\n",
    "\n",
    "information=[]\n",
    "for i in range(len(X_te[correct[3]])):\n",
    "    if X_te[correct[3]][i]!=0:\n",
    "        information.append(X_te[correct[3]][i])\n",
    "information=[index_to_word[w] for w in information]\n",
    "    \n",
    "print(\"Stock news: %s\" %information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First bad example\n",
      "Actual Return: 1\n",
      "Predicted Return: 0\n",
      "Stock news: ['Campbell', 'Soup', \"'s\", 'profit', 'beats', 'expectations', 'Nov', '22', 'Campbell', 'Soup', 'Co', 'the', 'world', \"'s\", 'largest', 'UNKNOWN_TOKEN', 'reported', 'a', 'better-than-expected', 'quarterly', 'profit', 'on', 'Tuesday', 'helped', 'by', 'cost-cutting', 'and', 'lower', 'commodity', 'prices', '.']\n"
     ]
    }
   ],
   "source": [
    "print(\"First bad example\")\n",
    "print(\"Actual Return: %s\" % y_te[incorrect[3]])\n",
    "print(\"Predicted Return: %s\" % prediction_2[incorrect[3]])\n",
    "\n",
    "information=[]\n",
    "for i in range(len(X_te[incorrect[3]])):\n",
    "    if X_te[incorrect[3]][i]!=0:\n",
    "        information.append(X_te[incorrect[3]][i])\n",
    "information=[index_to_word[w] for w in information]\n",
    "    \n",
    "print(\"Stock news: %s\" %information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second bad example\n",
      "Actual Return: 1\n",
      "Predicted Return: 0\n",
      "Stock news: ['BRIEF-Barclays', 'Bank', 'begins', 'note', 'buyback', 'LONDON', 'March', '27', 'Barclays', 'Bank', 'PLC', ':', '*', 'Offer', 'purchase', 'notes', 'for', 'cash', 'UNKNOWN_TOKEN', 'TO', 'UNKNOWN_TOKEN', 'NOTES', 'FOR', 'CASH', 'UNKNOWN_TOKEN', 'A', 'UNKNOWN_TOKEN', '``', 'UNKNOWN_TOKEN', 'UNKNOWN_TOKEN', \"''\", 'UNKNOWN_TOKEN']\n"
     ]
    }
   ],
   "source": [
    "print(\"Second bad example\")\n",
    "print(\"Actual Return: %s\" % y_te[incorrect[4]])\n",
    "print(\"Predicted Return: %s\" % prediction_2[incorrect[4]])\n",
    "\n",
    "information=[]\n",
    "for i in range(len(X_te[incorrect[4]])):\n",
    "    if X_te[incorrect[4]][i]!=0:\n",
    "        information.append(X_te[incorrect[4]][i])\n",
    "information=[index_to_word[w] for w in information]\n",
    "    \n",
    "print(\"Stock news: %s\" %information)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3: RNN+CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 129)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 129, 200)     6000000     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 129, 200)     6000000     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 128, 50)      20050       embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 129, 100)     120400      embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 64, 50)       0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 12900)        0           lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 3200)         0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 16100)        0           flatten_2[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 100)          1610100     concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1)            101         dense_5[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 13,750,651\n",
      "Trainable params: 13,750,651\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Concatenate\n",
    "from keras.models import Model, Input\n",
    "\n",
    "input_data=Input(shape=(max_length,))\n",
    "rnn_embedding=Embedding(input_dim=vocabulary_size,output_dim=200,\n",
    "                        input_length=max_length)(input_data)\n",
    "\n",
    "cnn_embedding=Embedding(input_dim=vocabulary_size,output_dim=200,\n",
    "                        input_length=max_length)(input_data)\n",
    "\n",
    "rnn_1=LSTM(100,return_sequences=True,recurrent_dropout=0.1)(rnn_embedding)\n",
    "rnn_2=Flatten()(rnn_1)\n",
    "cnn_1=Conv1D(filters = 50, kernel_size = 2, strides = 1, padding = 'valid')(cnn_embedding)\n",
    "cnn_2=MaxPooling1D(2, padding = 'valid')(cnn_1)\n",
    "cnn_3=Flatten()(cnn_2)\n",
    "\n",
    "combine=Concatenate(axis=-1)([rnn_2, cnn_3])\n",
    "\n",
    "model_3=Dense(100, activation='relu')(combine)\n",
    "out=Dense(1,activation=\"sigmoid\")(model_3)\n",
    "\n",
    "model_3=Model(input_data,out)\n",
    "\n",
    "model_3.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "print(model_3.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19468 samples, validate on 2164 samples\n",
      "Epoch 1/4\n",
      "19468/19468 [==============================] - 98s 5ms/step - loss: 0.7008 - acc: 0.5284 - val_loss: 0.6779 - val_acc: 0.5716\n",
      "Epoch 2/4\n",
      "19468/19468 [==============================] - 96s 5ms/step - loss: 0.5273 - acc: 0.7374 - val_loss: 0.6710 - val_acc: 0.6331\n",
      "Epoch 3/4\n",
      "19468/19468 [==============================] - 98s 5ms/step - loss: 0.2659 - acc: 0.9019 - val_loss: 0.7384 - val_acc: 0.6520\n",
      "Epoch 4/4\n",
      "19468/19468 [==============================] - 98s 5ms/step - loss: 0.1552 - acc: 0.9491 - val_loss: 0.7887 - val_acc: 0.6497\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c064069f60>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_3.fit(X_tr, y_tr, batch_size=200, epochs=4, validation_split=0.1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRECISION: 0.256\n",
      "RECALL: 0.506\n",
      "F1: 0.340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jiach\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\jiach\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "prediction_3=model_3.predict(X_te)\n",
    "prediction_3=np.argmax(prediction_3,axis=-1)\n",
    "\n",
    "precision_3 = precision_score(y_true=y_te, y_pred=prediction_3, average='weighted')\n",
    "recall_3 = recall_score(y_true=y_te, y_pred=prediction_3, average='weighted')\n",
    "f1_3 = f1_score(y_true=y_te, y_pred=prediction_3, average='weighted')\n",
    "\n",
    "print(\"PRECISION: {:.3f}\".format(precision_3))\n",
    "print(\"RECALL: {:.3f}\".format(recall_3))\n",
    "print(\"F1: {:.3f}\".format(f1_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First good example\n",
      "Actual Return: 0\n",
      "Predicted Return: 0\n",
      "Stock news: ['UBS', 'axes', '3', '500', 'jobs', 'in', 'cost-cutting', 'push', 'ZURICH', 'Switzerland', \"'s\", 'biggest', 'bank', 'UBS', 'AG', 'is', 'axe', '3', '500', 'jobs', 'shave', '2', 'billion', 'Swiss', 'francs', '(', '$', '2.5', 'billion', ')', 'off', 'annual', 'costs', 'as', 'it', 'joins', 'rival', 'investment', 'banks', 'in', 'reversing', 'the', 'post-crisis', 'hiring', 'binge', 'and', 'preparing', 'for', 'a', 'tough', 'few', 'years', '.', '|', 'Video']\n"
     ]
    }
   ],
   "source": [
    "correct=[]\n",
    "incorrect=[]\n",
    "\n",
    "for i in range(len(y_te)):\n",
    "    if prediction_3[i]==y_te[i]:\n",
    "        correct.append(i)\n",
    "    else:\n",
    "        incorrect.append(i)\n",
    "        \n",
    "print(\"First good example\")\n",
    "print(\"Actual Return: %s\" % y_te[correct[5]])\n",
    "print(\"Predicted Return: %s\" % prediction_3[correct[5]])\n",
    "\n",
    "information=[]\n",
    "for i in range(len(X_te[correct[5]])):\n",
    "    if X_te[correct[5]][i]!=0:\n",
    "        information.append(X_te[correct[5]][i])\n",
    "information=[index_to_word[w] for w in information]\n",
    "    \n",
    "print(\"Stock news: %s\" %information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First good example\n",
      "Actual Return: 0\n",
      "Predicted Return: 0\n",
      "Stock news: ['UPDATE', 'UNKNOWN_TOKEN', 'brews', 'in', 'Gulf', 'of', 'Mexico', 'as', 'energy', 'ops', 'resume', '*', 'Lingering', 'bad', 'weather', 'slowed', 'restart', 'efforts', 'post-Lee']\n"
     ]
    }
   ],
   "source": [
    "print(\"First good example\")\n",
    "print(\"Actual Return: %s\" % y_te[correct[6]])\n",
    "print(\"Predicted Return: %s\" % prediction_3[correct[6]])\n",
    "\n",
    "information=[]\n",
    "for i in range(len(X_te[correct[6]])):\n",
    "    if X_te[correct[6]][i]!=0:\n",
    "        information.append(X_te[correct[6]][i])\n",
    "information=[index_to_word[w] for w in information]\n",
    "    \n",
    "print(\"Stock news: %s\" %information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First bad example\n",
      "Actual Return: 1\n",
      "Predicted Return: 0\n",
      "Stock news: ['Fitch', ':', 'Barclays', \"'\", 'Retail', 'and', 'Cards', 'Businesses', 'UNKNOWN_TOKEN', 'Weak', 'Investment', 'Bank', '(', 'The', 'following', 'statement', 'was', 'released', 'by', 'the', 'rating', 'agency', ')', 'LONDON', 'October', '31', '(', 'Fitch', ')', 'Fitch', 'Ratings', 'says', 'the', 'UNKNOWN_TOKEN', 'of', 'Barclays', 'Plc', \"'s\", '(', 'Barclays', 'A/Stable/a', ')', 'investment', 'bank', '(', 'IB', ')', 'in', 'UNKNOWN_TOKEN', 'where', 'pre-tax', 'profit', 'fell', '39', '%', 'yoy', 'was', 'compensated', 'by', 'the', 'solid', 'performance', 'of', 'its', 'other', 'core', 'businesses', 'in', 'personal', 'and', 'corporate', 'banking', '(', 'UNKNOWN_TOKEN', ')', 'Barclaycard', 'and', 'Africa', 'Banking', '.', 'The', 'results', 'have', 'no', 'immediate', 'effect', 'on', 'Barclays', \"'\", 'ratings', '.', 'Results', 'in', '3Q', 'were', 'affected', 'by', 'changes', 'in', 'provisions', 'for', 'UNKNOWN_TOKEN']\n"
     ]
    }
   ],
   "source": [
    "print(\"First bad example\")\n",
    "print(\"Actual Return: %s\" % y_te[incorrect[5]])\n",
    "print(\"Predicted Return: %s\" % prediction_3[incorrect[5]])\n",
    "\n",
    "information=[]\n",
    "for i in range(len(X_te[incorrect[5]])):\n",
    "    if X_te[incorrect[5]][i]!=0:\n",
    "        information.append(X_te[incorrect[5]][i])\n",
    "information=[index_to_word[w] for w in information]\n",
    "    \n",
    "print(\"Stock news: %s\" %information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second bad example\n",
      "Actual Return: 1\n",
      "Predicted Return: 0\n",
      "Stock news: ['Deals', 'of', 'the', 'day-', 'Mergers', 'and', 'acquisitions', 'Oct', '14', 'The', 'following', 'bids', 'mergers', 'acquisitions', 'and', 'disposals', 'were', 'reported', 'by', '2000', 'GMT', 'on', 'Wednesday', ':']\n"
     ]
    }
   ],
   "source": [
    "print(\"Second bad example\")\n",
    "print(\"Actual Return: %s\" % y_te[incorrect[6]])\n",
    "print(\"Predicted Return: %s\" % prediction_3[incorrect[6]])\n",
    "\n",
    "information=[]\n",
    "for i in range(len(X_te[incorrect[6]])):\n",
    "    if X_te[incorrect[6]][i]!=0:\n",
    "        information.append(X_te[incorrect[6]][i])\n",
    "information=[index_to_word[w] for w in information]\n",
    "    \n",
    "print(\"Stock news: %s\" %information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model      Precision    Recall    f1_score\n",
      "-------  -----------  --------  ----------\n",
      "RNN         0.255913  0.505879    0.339885\n",
      "CNN         0.255913  0.505879    0.339885\n",
      "RNN+CNN     0.255913  0.505879    0.339885\n"
     ]
    }
   ],
   "source": [
    "# Model comparision\n",
    "\n",
    "from tabulate import tabulate\n",
    "print (tabulate([['RNN',precision_1, recall_1,f1_1],\n",
    "                 ['CNN',precision_2, recall_2,f1_2],\n",
    "                 ['RNN+CNN',precision_3,recall_3,f1_3]],\n",
    "                headers=['Model','Precision','Recall', 'f1_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
